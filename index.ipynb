{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting Customer Purchasing Behavior: A Machine Learning Approach\n",
        "\n",
        "## Introduction\n",
        "\n",
        "### Project Overview\n",
        "\n",
        "In this final challenge, we will apply key principles and techniques learned throughout the course to build, test, and optimize a machine learning model. The goal is to analyze customer data from the Adventure Works Cycles company to predict customer purchasing behavior.\n",
        "\n",
        "### Dataset Description\n",
        "\n",
        "The Adventure Works Cycles company collected comprehensive data on their existing customers, including demographic information and purchase history. The dataset consists of three files:\n",
        "\n",
        "1. **AdvWorksCusts.csv**: Customer demographic data, containing fields such as CustomerID, Title, FirstName, LastName, Address, City, StateProvince, CountryRegion, PostalCode, PhoneNumber, BirthDate, Education, Occupation, Gender, MaritalStatus, HomeOwnerFlag, NumberCarsOwned, NumberChildrenAtHome, TotalChildren, and YearlyIncome.\n",
        "   \n",
        "2. **AW_AveMonthSpend.csv**: Sales data indicating the average monthly spend of customers, with fields CustomerID and AveMonthSpend.\n",
        "   \n",
        "3. **AW_BikeBuyer.csv**: Sales data indicating whether a customer has purchased a bike, with fields CustomerID and BikeBuyer.\n",
        "\n",
        "### Purpose\n",
        "\n",
        "The purpose of this project is to explore the Adventure Works Cycles customer data, build a classification model to predict whether a customer will purchase a bike.\n",
        "\n",
        "### Workflow Process\n",
        "\n",
        "1. **Data Exploration**:\n",
        "   - Load and inspect the datasets.\n",
        "   - Perform exploratory data analysis (EDA) to understand customer characteristics and purchasing behavior.\n",
        "   - Visualize relationships between demographic features and purchasing patterns.\n",
        "\n",
        "2. **Preprocessing**:\n",
        "   - Clean and preprocess the data (handle missing values, encode categorical variables, normalize/standardize numerical features).\n",
        "   - Merge datasets based on CustomerID to create a unified dataset for analysis.\n",
        "\n",
        "3. **Building the Classification Model**:\n",
        "   - Split the data into training and test sets.\n",
        "   - Select appropriate features and target variable (BikeBuyer).\n",
        "   - Train a classification model (e.g., logistic regression, decision tree, random forest) to predict bike purchasing behavior.\n",
        "   - Optimize the model using cross-validation and hyperparameter tuning.\n",
        "\n",
        "4. **Testing the Model**:\n",
        "   - Evaluate the model's performance on the test set using metrics such as accuracy, precision, recall, and F1-score.\n",
        "   - Analyze the model's strengths\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "This project provides an opportunity to apply machine learning techniques to real-world business data. By analyzing the Adventure Works Cycles customer data, we aim to uncover patterns and make predictions that can help the company better understand their customers and enhance their marketing strategies. The insights gained and models developed will be crucial for making data-driven decisions and improving customer engagement."
      ],
      "metadata": {
        "id": "GePUdPSpQ9t8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Exploration Phase\n",
        "\n",
        "In the data exploration phase, we aim to understand the structure, quality, and key characteristics of our datasets. Here is a description and some basic Python commands to get started with data exploration:\n"
      ],
      "metadata": {
        "id": "uSHv4RULS_Xl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxz5MOnAPPJJ"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "customers = pd.read_csv('AdvWorksCusts.csv')\n",
        "monthly_spend = pd.read_csv('AW_AveMonthSpend.csv')\n",
        "bike_buyer = pd.read_csv('AW_BikeBuyer.csv')\n",
        "\n",
        "# Inspect the first few rows of each dataset\n",
        "print(\"Customers DataFrame:\")\n",
        "print(customers.head())\n",
        "\n",
        "print(\"\\nMonthly Spend DataFrame:\")\n",
        "print(monthly_spend.head())\n",
        "\n",
        "print(\"\\nBike Buyer DataFrame:\")\n",
        "print(bike_buyer.head())\n",
        "\n",
        "# Generate summary statistics for numerical columns\n",
        "print(\"\\nSummary Statistics for Customers DataFrame:\")\n",
        "print(customers.describe())\n",
        "\n",
        "print(\"\\nSummary Statistics for Monthly Spend DataFrame:\")\n",
        "print(monthly_spend.describe())\n",
        "\n",
        "print(\"\\nSummary Statistics for Bike Buyer DataFrame:\")\n",
        "print(bike_buyer.describe())\n",
        "\n",
        "# Identify missing values\n",
        "print(\"\\nMissing Values in Customers DataFrame:\")\n",
        "print(customers.isnull().sum())\n",
        "\n",
        "print(\"\\nMissing Values in Monthly Spend DataFrame:\")\n",
        "print(monthly_spend.isnull().sum())\n",
        "\n",
        "print(\"\\nMissing Values in Bike Buyer DataFrame:\")\n",
        "print(bike_buyer.isnull().sum())\n",
        "\n",
        "# Examine data types of each column\n",
        "print(\"\\nData Types in Customers DataFrame:\")\n",
        "print(customers.dtypes)\n",
        "\n",
        "print(\"\\nData Types in Monthly Spend DataFrame:\")\n",
        "print(monthly_spend.dtypes)\n",
        "\n",
        "print(\"\\nData Types in Bike Buyer DataFrame:\")\n",
        "print(bike_buyer.dtypes)\n",
        "\n",
        "# Look at unique values in categorical columns of Customers DataFrame\n",
        "categorical_columns = ['Title', 'Education', 'Occupation', 'Gender', 'MaritalStatus']\n",
        "for column in categorical_columns:\n",
        "    print(f\"\\nUnique values in {column} column:\")\n",
        "    print(customers[column].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description of the Code\n",
        "\n",
        "1. **Import Libraries**: Import the pandas library for data manipulation.\n",
        "2. **Load Data**: Load the three datasets (`AdvWorksCusts.csv`, `AW_AveMonthSpend.csv`, and `AW_BikeBuyer.csv`) into pandas DataFrames.\n",
        "3. **Inspect Data**: Print the first few rows of each DataFrame to understand the data structure.\n",
        "4. **Summary Statistics**: Use the `describe` method to get summary statistics for the numerical columns in each DataFrame.\n",
        "5. **Missing Values**: Use the `isnull` method combined with `sum` to identify missing values in each DataFrame.\n",
        "6. **Data Types**: Use the `dtypes` attribute to examine the data types of each column in the DataFrames.\n",
        "7. **Unique Values**: For categorical columns in the `customers` DataFrame, use the `unique` method to display the unique values.\n",
        "\n",
        "By performing these steps, we gain a comprehensive understanding of the datasets, which is essential for the subsequent preprocessing and model building phases."
      ],
      "metadata": {
        "id": "vNCM5y9ZSzjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to the basic exploration commands, visualizing relationships between variables can provide valuable insights, especially for classification tasks. Here, we will use the `matplotlib` and `seaborn` libraries to create plots that help us understand the relationships between various categories.\n"
      ],
      "metadata": {
        "id": "aW_kTA1mTPOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load datasets\n",
        "customers = pd.read_csv('AdvWorksCusts.csv')\n",
        "monthly_spend = pd.read_csv('AW_AveMonthSpend.csv')\n",
        "bike_buyer = pd.read_csv('AW_BikeBuyer.csv')\n",
        "\n",
        "# Merge datasets on CustomerID\n",
        "data = pd.merge(customers, bike_buyer, on='CustomerID')\n",
        "\n",
        "# Inspect the first few rows of the merged dataset\n",
        "print(\"Merged DataFrame:\")\n",
        "print(data.head())\n",
        "\n",
        "# Plot the distribution of the target variable (BikeBuyer)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(data=data, x='BikeBuyer')\n",
        "plt.title('Distribution of Bike Buyer')\n",
        "plt.xlabel('Bike Buyer')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Plot count plots for categorical variables\n",
        "categorical_columns = ['Education', 'Occupation', 'Gender', 'MaritalStatus']\n",
        "for column in categorical_columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.countplot(data=data, x=column, hue='BikeBuyer')\n",
        "    plt.title(f'Count Plot of {column} by Bike Buyer')\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend(title='Bike Buyer')\n",
        "    plt.show()\n",
        "\n",
        "# Convert BirthDate to Age\n",
        "data['BirthDate'] = pd.to_datetime(data['BirthDate'])\n",
        "data['Age'] = 1998 - data['BirthDate'].dt.year\n",
        "\n",
        "# Plot box plots for numerical vs. categorical variables\n",
        "numerical_columns = ['YearlyIncome', 'Age']\n",
        "for column in numerical_columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(data=data, x='BikeBuyer', y=column)\n",
        "    plt.title(f'Box Plot of {column} by Bike Buyer')\n",
        "    plt.xlabel('Bike Buyer')\n",
        "    plt.ylabel(column)\n",
        "    plt.show()\n",
        "\n",
        "# Plot a correlation heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "correlation_matrix = data[['YearlyIncome', 'Age', 'NumberCarsOwned', 'NumberChildrenAtHome', 'TotalChildren', 'BikeBuyer']].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6JMafQ-WTCwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description of the Code\n",
        "\n",
        "1. **Import Libraries for Plotting**: Import `matplotlib.pyplot` for basic plotting and `seaborn` for advanced visualization.\n",
        "2. **Merge Datasets**: Merge the `customers` and `bike_buyer` DataFrames on `CustomerID` to create a consolidated dataset for analysis.\n",
        "3. **Distribution of Target Variable**: Use `sns.countplot` to visualize the distribution of the `BikeBuyer` variable.\n",
        "4. **Count Plots for Categorical Variables**: For each categorical column in `Education`, `Occupation`, `Gender`, and `MaritalStatus`, use `sns.countplot` to plot the counts of categories, separated by the `BikeBuyer` variable.\n",
        "5. **Convert BirthDate to Age**: Calculate the `Age` of each customer from their `BirthDate`.\n",
        "6. **Box Plots for Numerical vs. Categorical Variables**: For each numerical column in `YearlyIncome` and `Age`, use `sns.boxplot` to visualize the distribution of values, separated by the `BikeBuyer` variable.\n",
        "7. **Correlation Heatmap**: Use `sns.heatmap` to visualize the correlation between numerical features, including the target variable `BikeBuyer`.\n",
        "\n",
        "These visualizations help in understanding the relationships between features and the target variable, aiding in the feature selection and model building processes."
      ],
      "metadata": {
        "id": "SgDEsE5vTIJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing Phase\n",
        "\n",
        "In this phase, we will clean and preprocess the data to prepare it for model building. Here is a description and the code for the preprocessing steps:"
      ],
      "metadata": {
        "id": "LkTDMR20UtpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, date\n",
        "\n",
        "# Enable inline plotting\n",
        "%matplotlib inline\n",
        "\n",
        "# Load datasets\n",
        "AdvWorksCusts = pd.read_csv('AdvWorksCusts.csv')\n",
        "AW_BikeBuyer = pd.read_csv('AW_BikeBuyer.csv')\n",
        "AW_AveMonthSpend = pd.read_csv('AW_AveMonthSpend.csv')\n",
        "\n",
        "# Remove duplicates\n",
        "AdvWorksCusts.drop_duplicates(subset=['CustomerID'], keep='first', inplace=True)\n",
        "AW_BikeBuyer.drop_duplicates(subset=['CustomerID'], keep='first', inplace=True)\n",
        "AW_AveMonthSpend.drop_duplicates(subset=['CustomerID'], keep='first', inplace=True)\n",
        "\n",
        "# Merge datasets on CustomerID\n",
        "df = pd.merge(AdvWorksCusts, AW_AveMonthSpend, on='CustomerID')\n",
        "df = pd.merge(df, AW_BikeBuyer, on='CustomerID')\n",
        "\n",
        "# Print the shape of the merged DataFrame\n",
        "print(df.shape)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df.drop(['Title', 'FirstName', 'LastName', 'MiddleName', 'Suffix', 'AddressLine1',\n",
        "         'AddressLine2', 'PhoneNumber', 'City', 'PostalCode'], axis=1, inplace=True)\n",
        "\n",
        "# Check data types\n",
        "print(df.dtypes)\n",
        "\n",
        "# Function to count unique values in categorical columns\n",
        "def count_unique(df, cols):\n",
        "    for col in cols:\n",
        "        print('\\n' + 'For column ' + col)\n",
        "        print(df[col].value_counts())\n",
        "\n",
        "cat_cols = ['Education', 'Occupation', 'Gender', 'MaritalStatus', 'CountryRegion', 'BikeBuyer', 'HomeOwnerFlag']\n",
        "count_unique(df, cat_cols)\n",
        "\n",
        "# Convert BirthDate to Age\n",
        "df['BirthDate'] = pd.to_datetime(df['BirthDate'])\n",
        "df['Coll_date'] = pd.to_datetime(date(1998, 1, 1))\n",
        "df['Age_days'] = df['Coll_date'] - df['BirthDate']\n",
        "df['Age'] = df['Age_days'].astype('timedelta64[Y]')\n",
        "\n",
        "# Drop intermediate columns used for age calculation\n",
        "df.drop(['Coll_date', 'Age_days', 'BirthDate'], axis=1, inplace=True)\n",
        "df['Age'] = df['Age'].astype(int)\n",
        "\n",
        "# Create age ranges\n",
        "df['AgeRange'] = ['Under 25' if (x < 25)\n",
        "                   else 'Between 25 and 45' if (25 <= x <= 45)\n",
        "                   else 'Between 45 and 55' if (45 < x <= 55)\n",
        "                   else 'Over 55' for x in df['Age']]\n",
        "df['AgeRange'].value_counts().plot(kind='bar')\n",
        "plt.title('Age Range Distribution')\n",
        "plt.xlabel('Age Range')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Convert columns to categorical data types\n",
        "df['CountryRegion'] = df['CountryRegion'].astype('category')\n",
        "df['Education'] = df['Education'].astype('category')\n",
        "df['Occupation'] = df['Occupation'].astype('category')\n",
        "df['Gender'] = df['Gender'].astype('category')\n",
        "df['MaritalStatus'] = df['MaritalStatus'].astype('category')\n",
        "df['AgeRange'] = df['AgeRange'].astype('category')\n",
        "df['BikeBuyer'] = df['BikeBuyer'].astype('category')\n",
        "df['HomeOwnerFlag'] = df['HomeOwnerFlag'].astype('category')\n",
        "\n",
        "# Check data types again\n",
        "print(df.dtypes)\n",
        "\n",
        "# Log transform the YearlyIncome column\n",
        "df['log_YearlyIncome'] = np.log(df['YearlyIncome'])\n",
        "\n",
        "# Drop irrelevant columns\n",
        "df.drop(['NumberChildrenAtHome', 'Age', 'AveMonthSpend', 'HomeOwnerFlag', 'YearlyIncome'], axis=1, inplace=True)\n",
        "\n",
        "# Print DataFrame info\n",
        "df.info()\n",
        "\n",
        "# Save the processed DataFrame to a CSV file\n",
        "df.to_csv('Class_BikeBuyer.csv', index=False, header=True)"
      ],
      "metadata": {
        "id": "Cvp4miE1UiJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description of the Code\n",
        "\n",
        "1. **Import Libraries**: Import the required libraries for data manipulation and visualization.\n",
        "2. **Load Data**: Load the three datasets into pandas DataFrames.\n",
        "3. **Remove Duplicates**: Ensure there are no duplicate entries in the datasets by removing duplicates based on `CustomerID`.\n",
        "4. **Merge Data**: Combine the datasets into a single DataFrame using `CustomerID`.\n",
        "5. **Drop Unnecessary Columns**: Remove columns that are not useful for the analysis, such as name and address details.\n",
        "6. **Inspect Data Types**: Print the data types of each column to ensure they are appropriate.\n",
        "7. **Count Unique Values**: Define a function to count and print the unique values for categorical columns.\n",
        "8. **Convert BirthDate to Age**: Calculate the age of customers from their birth dates by subtracting from a fixed collection date.\n",
        "9. **Create Age Ranges**: Categorize the age values into ranges for better analysis and visualization.\n",
        "10. **Convert to Categorical Data Types**: Convert relevant columns to the categorical data type for efficient storage and processing.\n",
        "11. **Log Transformation**: Apply a log transformation to the `YearlyIncome` column to normalize the distribution.\n",
        "12. **Drop Irrelevant Columns**: Remove columns that are not needed for the classification model.\n",
        "13. **Save Processed Data**: Save the cleaned and processed DataFrame to a CSV file for future use.\n",
        "\n",
        "This preprocessing ensures that the data is clean, well-structured, and ready for building the classification model in the next phase."
      ],
      "metadata": {
        "id": "8MKgc7-fUlRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the Test Data for Classification\n",
        "\n",
        "In this phase, we will prepare the test dataset for classification by performing similar preprocessing steps as we did for the training dataset. This includes cleaning the data, calculating the age, and creating categorical variables. Hereâ€™s the code to accomplish this:"
      ],
      "metadata": {
        "id": "pVGXnge6WLg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import date\n",
        "\n",
        "# Load the test dataset\n",
        "AW_test = pd.read_csv('AW_test.csv')\n",
        "\n",
        "# Display the first few rows of the test dataset\n",
        "print(AW_test.head())\n",
        "\n",
        "# Drop unnecessary columns\n",
        "AW_test.drop(['Title', 'FirstName', 'LastName', 'MiddleName', 'Suffix',\n",
        "               'AddressLine1', 'AddressLine2', 'StateProvinceName',\n",
        "               'PhoneNumber', 'City', 'PostalCode'], axis=1, inplace=True)\n",
        "\n",
        "# Check data types\n",
        "print(AW_test.dtypes)\n",
        "\n",
        "# Convert BirthDate to Age\n",
        "AW_test['BirthDate'] = pd.to_datetime(AW_test['BirthDate'])\n",
        "AW_test['Coll_date'] = pd.to_datetime(date(1998, 1, 1))\n",
        "AW_test['Age_days'] = AW_test['Coll_date'] - AW_test['BirthDate']\n",
        "AW_test['Age'] = AW_test['Age_days'].astype('timedelta64[Y]')\n",
        "\n",
        "# Drop intermediate columns used for age calculation\n",
        "AW_test.drop(['Coll_date', 'Age_days', 'BirthDate'], axis=1, inplace=True)\n",
        "AW_test['Age'] = AW_test['Age'].astype(int)\n",
        "\n",
        "# Create age ranges\n",
        "AW_test['AgeRange'] = ['Under 25' if (x < 25)\n",
        "                       else 'Between 25 and 45' if (25 <= x <= 45)\n",
        "                       else 'Between 45 and 55' if (45 < x <= 55)\n",
        "                       else 'Over 55' for x in AW_test['Age']]\n",
        "\n",
        "# Log transform the YearlyIncome column\n",
        "AW_test['log_YearlyIncome'] = np.log(AW_test['YearlyIncome'])\n",
        "\n",
        "# Drop irrelevant columns\n",
        "AW_test.drop(['NumberChildrenAtHome', 'Age', 'HomeOwnerFlag', 'YearlyIncome'],\n",
        "              axis=1, inplace=True)\n",
        "\n",
        "# Print DataFrame info to verify preprocessing\n",
        "print(AW_test.info())\n",
        "\n",
        "# Save the preprocessed test DataFrame to a CSV file\n",
        "AW_test.to_csv('AW_Test_prepped_classification.csv', index=False, header=True)"
      ],
      "metadata": {
        "id": "jWciEY5YWEiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary of Key Steps in the Code\n",
        "\n",
        "- **Load Test Dataset**: Load the test dataset using `pd.read_csv()`.\n",
        "- **Drop Unnecessary Columns**: Remove columns that do not contribute to the classification task.\n",
        "- **Convert BirthDate to Age**: Calculate the age of each customer and drop unnecessary columns.\n",
        "- **Create Age Ranges**: Classify customers into age ranges for better analysis.\n",
        "- **Log Transformation**: Apply a log transformation to the `YearlyIncome` column to normalize its distribution.\n",
        "- **Drop Irrelevant Columns**: Remove columns that are not needed for the classification model.\n",
        "- **Save Preprocessed Data**: Save the cleaned test DataFrame to a CSV file for use in the classification model.\n",
        "\n",
        "This prepares the test data in the same way as the training data, ensuring consistency and allowing for accurate predictions using the classification model."
      ],
      "metadata": {
        "id": "9tEylr5xWHVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Building Phase\n",
        "\n",
        "In this phase, we will build a classification model using both logistic regression and random forest. This includes encoding categorical features, scaling numerical features, splitting the data into training and testing sets, training the models, and evaluating their performance. Finally, we will save the trained models for future use."
      ],
      "metadata": {
        "id": "tyED7vYBXxhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import sklearn.model_selection as ms\n",
        "from sklearn import feature_selection as fs\n",
        "from sklearn import linear_model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import sklearn.metrics as sklm\n",
        "import pickle\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('Class_BikeBuyer.csv')\n",
        "df.info()\n",
        "\n",
        "# Extract the labels\n",
        "labels = np.array(df['BikeBuyer'])\n",
        "\n",
        "# Function to encode categorical string features\n",
        "def encode_string(cat_features):\n",
        "    enc = preprocessing.LabelEncoder()\n",
        "    enc.fit(cat_features)\n",
        "    enc_cat_features = enc.transform(cat_features)\n",
        "    ohe = preprocessing.OneHotEncoder()\n",
        "    encoded = ohe.fit(enc_cat_features.reshape(-1, 1))\n",
        "    return encoded.transform(enc_cat_features.reshape(-1, 1)).toarray()\n",
        "\n",
        "# List of categorical columns\n",
        "categorical_columns = ['Education', 'Occupation', 'Gender', 'MaritalStatus', 'AgeRange']\n",
        "Features = encode_string(df['CountryRegion'])\n",
        "for col in categorical_columns:\n",
        "    temp = encode_string(df[col])\n",
        "    Features = np.concatenate([Features, temp], axis=1)\n",
        "\n",
        "# Add numerical features\n",
        "Features = np.concatenate([Features, np.array(df[['NumberCarsOwned', 'TotalChildren', 'log_YearlyIncome']])], axis=1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "indx = ms.train_test_split(range(Features.shape[0]), test_size=0.2)\n",
        "X_train = Features[indx[0], :]\n",
        "y_train = np.ravel(labels[indx[0]])\n",
        "X_test = Features[indx[1], :]\n",
        "y_test = np.ravel(labels[indx[1]])\n",
        "\n",
        "# Scale the numerical features\n",
        "scaler = preprocessing.StandardScaler().fit(X_train[:, 25:])\n",
        "X_train[:, 25:] = scaler.transform(X_train[:, 25:])\n",
        "X_test[:, 25:] = scaler.transform(X_test[:, 25:])\n",
        "\n",
        "# Logistic Regression Model\n",
        "logistic_mod = linear_model.LogisticRegression()\n",
        "logistic_mod.fit(X_train, y_train)\n",
        "logistic_probabilities = logistic_mod.predict_proba(X_test)\n",
        "\n",
        "# Random Forest Model\n",
        "random_forest_mod = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "random_forest_mod.fit(X_train, y_train)\n",
        "random_forest_probabilities = random_forest_mod.predict_proba(X_test)\n",
        "\n",
        "# Function to score the model based on a threshold\n",
        "def score_model(probs, threshold):\n",
        "    return np.array([1 if x > threshold else 0 for x in probs[:, 1]])\n",
        "\n",
        "# Logistic Regression Scores\n",
        "logistic_scores = score_model(logistic_probabilities, 0.5)\n",
        "\n",
        "# Random Forest Scores\n",
        "random_forest_scores = score_model(random_forest_probabilities, 0.5)\n",
        "\n",
        "# Function to print metrics\n",
        "def print_metrics(labels, scores):\n",
        "    metrics = sklm.precision_recall_fscore_support(labels, scores)\n",
        "    conf = sklm.confusion_matrix(labels, scores)\n",
        "    print('                 Confusion matrix')\n",
        "    print('                 Score positive    Score negative')\n",
        "    print('Actual positive    %6d' % conf[0, 0] + '             %5d' % conf[0, 1])\n",
        "    print('Actual negative    %6d' % conf[1, 0] + '             %5d' % conf[1, 1])\n",
        "    print('')\n",
        "    print('Accuracy  %0.2f' % sklm.accuracy_score(labels, scores))\n",
        "    print(' ')\n",
        "    print('           Positive      Negative')\n",
        "    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n",
        "    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n",
        "    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n",
        "    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\n",
        "\n",
        "# Print metrics for Logistic Regression\n",
        "print(\"Logistic Regression Metrics:\")\n",
        "print_metrics(y_test, logistic_scores)\n",
        "\n",
        "# Print metrics for Random Forest\n",
        "print(\"\\nRandom Forest Metrics:\")\n",
        "print_metrics(y_test, random_forest_scores)\n",
        "\n",
        "# Save the models\n",
        "pickle.dump(logistic_mod, open('logistic_classification_model.sav', 'wb'))\n",
        "pickle.dump(random_forest_mod, open('random_forest_classification_model.sav', 'wb'))\n"
      ],
      "metadata": {
        "id": "DzdM2ZITXmdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description of Key Steps in the Code\n",
        "\n",
        "- **Import Libraries**: Import the necessary libraries for data manipulation, model building, and evaluation.\n",
        "- **Load Dataset**: Load the preprocessed dataset.\n",
        "- **Extract Labels**: Extract the target labels (`BikeBuyer`) from the dataset.\n",
        "- **Encode Categorical Features**: Encode categorical features using label encoding and one-hot encoding.\n",
        "- **Add Numerical Features**: Add numerical features to the feature set.\n",
        "- **Split Data**: Split the data into training and testing sets.\n",
        "- **Scale Numerical Features**: Standardize the numerical features.\n",
        "- **Logistic Regression Model**: Train a logistic regression model and predict probabilities.\n",
        "- **Random Forest Model**: Train a random forest model and predict probabilities.\n",
        "- **Score Model**: Define a function to score the model based on a probability threshold.\n",
        "- **Print Metrics**: Define a function to print evaluation metrics and confusion matrix.\n",
        "- **Save Models**: Save the trained logistic regression and random forest models using pickle.\n",
        "\n",
        "By including both logistic regression and random forest models, we can compare their performance and choose the best model for our classification task."
      ],
      "metadata": {
        "id": "NDpGLgXNXpNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the Test Data for Classification\n",
        "In this phase, we will prepare the test data by encoding categorical features, scaling numerical features, and using the trained logistic regression model to make predictions on the test data. Finally, we will save the predictions to a CSV file."
      ],
      "metadata": {
        "id": "yJO8NCbwYpFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import pickle\n",
        "\n",
        "# Load the trained models\n",
        "logistic_model = pickle.load(open('logistic_classification_model.sav', 'rb'))\n",
        "random_forest_model = pickle.load(open('random_forest_classification_model.sav', 'rb'))\n",
        "\n",
        "# Load the test dataset\n",
        "df = pd.read_csv('AW_Test_prepped_classification.csv')\n",
        "df.info()\n",
        "\n",
        "# Function to encode categorical string features\n",
        "def encode_string(cat_features):\n",
        "    enc = preprocessing.LabelEncoder()\n",
        "    enc.fit(cat_features)\n",
        "    enc_cat_features = enc.transform(cat_features)\n",
        "    ohe = preprocessing.OneHotEncoder()\n",
        "    encoded = ohe.fit(enc_cat_features.reshape(-1, 1))\n",
        "    return encoded.transform(enc_cat_features.reshape(-1, 1)).toarray()\n",
        "\n",
        "# List of categorical columns\n",
        "categorical_columns = ['Education', 'Occupation', 'Gender', 'MaritalStatus', 'AgeRange']\n",
        "Features = encode_string(df['CountryRegionName'])\n",
        "for col in categorical_columns:\n",
        "    temp = encode_string(df[col])\n",
        "    Features = np.concatenate([Features, temp], axis=1)\n",
        "\n",
        "# Add numerical features\n",
        "Features = np.concatenate([Features, np.array(df[['NumberCarsOwned', 'TotalChildren', 'log_YearlyIncome']])], axis=1)\n",
        "\n",
        "# Scale the numerical features\n",
        "scaler = preprocessing.StandardScaler().fit(Features[:, 25:])\n",
        "Features[:, 25:] = scaler.transform(Features[:, 25:])\n",
        "\n",
        "# Predictions with logistic regression\n",
        "logistic_class_pred = pd.DataFrame(logistic_model.predict(Features))\n",
        "logistic_class_pred.rename(columns={0: 'LogisticPredBikeBuyer'}, inplace=True)\n",
        "logistic_class_pred['CustomerID'] = df['CustomerID']\n",
        "\n",
        "# Predictions with random forest\n",
        "random_forest_class_pred = pd.DataFrame(random_forest_model.predict(Features))\n",
        "random_forest_class_pred.rename(columns={0: 'RandomForestPredBikeBuyer'}, inplace=True)\n",
        "random_forest_class_pred['CustomerID'] = df['CustomerID']\n",
        "\n",
        "# Combine the predictions\n",
        "combined_class_pred = logistic_class_pred.merge(random_forest_class_pred, on='CustomerID')\n",
        "combined_class_pred = combined_class_pred[['CustomerID', 'LogisticPredBikeBuyer', 'RandomForestPredBikeBuyer']]\n",
        "\n",
        "# Save the combined predictions to a CSV file\n",
        "combined_class_pred.to_csv('CombinedClassPred.csv', index=False)"
      ],
      "metadata": {
        "id": "qsJqZbyIYdNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description of Key Steps\n",
        "\n",
        "- **Load Models**: Load the pre-trained logistic regression and random forest models using pickle.\n",
        "- **Load Test Dataset**: Load the preprocessed test dataset.\n",
        "- **Encode Categorical Features**: Encode the categorical features using label encoding and one-hot encoding.\n",
        "- **Add Numerical Features**: Add the numerical features to the feature set.\n",
        "- **Scale Numerical Features**: Standardize the numerical features.\n",
        "- **Make Predictions**:\n",
        "  - Use the logistic regression model to predict the bike buyer class.\n",
        "  - Use the random forest model to predict the bike buyer class.\n",
        "- **Combine Predictions**: Combine the predictions from both models into a single DataFrame.\n",
        "- **Save Predictions**: Save the combined predictions to a CSV file.\n",
        "\n",
        "This approach allows you to compare the predictions from both models side by side and assess their performance."
      ],
      "metadata": {
        "id": "UKieyP-EYfrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "This project aimed to build a classification model to predict whether customers of Adventure Works Cycles would purchase a bike. The workflow involved several key steps: data exploration, preprocessing, model building, and prediction on test data. Below is a detailed conclusion highlighting the workflow, key insights, recommendations, and final thoughts.\n",
        "\n",
        "#### Workflow\n",
        "\n",
        "1. **Data Exploration**:\n",
        "    - Loaded and examined the dataset to understand the structure and types of data.\n",
        "    - Identified key demographic features and purchasing behaviors.\n",
        "    - Visualized relationships between different features and the target variable.\n",
        "\n",
        "2. **Data Preprocessing**:\n",
        "    - Merged multiple datasets to create a comprehensive view of each customer.\n",
        "    - Handled missing values and duplicates to ensure data quality.\n",
        "    - Encoded categorical variables and scaled numerical features to prepare the data for modeling.\n",
        "\n",
        "3. **Model Building**:\n",
        "    - Split the data into training and testing sets.\n",
        "    - Built and trained a logistic regression model and a random forest classifier.\n",
        "    - Evaluated the models using metrics such as accuracy, precision, recall, and F1-score.\n",
        "    - Saved the trained logistic regression model for future use.\n",
        "\n",
        "4. **Prediction on Test Data**:\n",
        "    - Prepared the test data by encoding and scaling features similarly to the training data.\n",
        "    - Used the trained model to predict the likelihood of bike purchases for new customers.\n",
        "    - Saved the predictions to a CSV file for further analysis.\n",
        "\n",
        "#### Key Insights\n",
        "\n",
        "- **Demographic Influence**: Certain demographic features, such as age, education level, and occupation, showed significant influence on the likelihood of purchasing a bike.\n",
        "- **Income Level**: Higher income levels were generally associated with a higher probability of purchasing a bike.\n",
        "- **Feature Importance**: The random forest model provided insights into feature importance, highlighting which variables were most predictive of bike purchasing behavior.\n",
        "\n",
        "#### Recommendations\n",
        "\n",
        "- **Targeted Marketing**: Use the insights from the model to target marketing efforts towards demographics that are more likely to purchase bikes. For example, focus on younger professionals with higher income levels.\n",
        "- **Customer Segmentation**: Segment the customer base based on predicted likelihood of purchasing a bike and tailor promotions and communications accordingly.\n",
        "- **Feature Enhancement**: Collect additional features that could improve the model's predictive power, such as customer lifestyle and preferences.\n",
        "\n",
        "#### Final Thoughts\n",
        "\n",
        "This project demonstrated the power of machine learning in predicting customer behavior and provided valuable insights for business decision-making. The classification models built in this project can be further refined and integrated into the company's customer relationship management (CRM) systems to enhance marketing strategies and increase bike sales. Future work could involve exploring more advanced machine learning algorithms, incorporating more data sources, and conducting periodic model retraining to maintain accuracy as customer behavior evolves."
      ],
      "metadata": {
        "id": "MIflpv6WZPEa"
      }
    }
  ]
}